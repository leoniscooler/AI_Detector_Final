# -*- coding: utf-8 -*-
"""Cosmos_Art_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18NZptbwC_50s8zxwip5K3BeQNVHGZaWX
"""

import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import cv2
from PIL import Image
import io
import random
import os
import time

# Set random seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_seed()

# Define the model architecture
class PatchCraftDetector(nn.Module):
    def __init__(self, patch_size=64):
        super(PatchCraftDetector, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Flatten()
        )

        # Dynamic feature size calculation
        with torch.no_grad():
            dummy = torch.zeros(1, 3, patch_size, patch_size)
            dummy_out = self.features(dummy)
            in_features = dummy_out.size(1)

        self.classifier = nn.Sequential(
            nn.Linear(in_features, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, 2)
        )

    def forward(self, x):
        x = self.features(x)
        return self.classifier(x)

@st.cache_resource
def load_model():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = PatchCraftDetector(patch_size=64).to(device)

    model_path = 'Cosmos_Art_Detector.pth'
    if os.path.exists(model_path):
        try:
            model.load_state_dict(torch.load(model_path, map_location=device))
            model.eval()
            return model
        except Exception as e:
            st.error(f"Error loading model: {e}")
            return None
    else:
        st.error(f"Model file not found at {model_path}")
        return None

def preprocess_image(image):
    """Convert uploaded image to normalized tensor"""
    try:
        # Convert to RGB if needed
        if image.mode == 'RGBA':
            image = image.convert('RGB')
        elif image.mode == 'L':
            image = image.convert('RGB')

        image = np.array(image)
        image = cv2.resize(image, (128, 128))
        image = image.astype(np.float32) / 255.0
        image = np.transpose(image, (2, 0, 1))  # CHW format
        return image
    except Exception as e:
        st.error(f"Error processing image: {e}")
        return None

def predict_image(image_bytes, num_patches=50, seed=42):
    """Make prediction on uploaded image"""
    set_seed(seed)
    img = Image.open(io.BytesIO(image_bytes))
    img_tensor = preprocess_image(img)
    if img_tensor is None:
        return None, None, None

    patch_size = 64
    max_pos = 128 - patch_size

    # Generate fixed patch locations
    patch_locations = [(np.random.randint(0, max_pos),
                       np.random.randint(0, max_pos))
                      for _ in range(num_patches)]

    # Extract patches
    patches = []
    for x, y in patch_locations:
        patch = img_tensor[:, x:x+patch_size, y:y+patch_size]
        patches.append(patch)

    patches = torch.tensor(np.array(patches), dtype=torch.float32).to(device)

    # Predict
    with torch.no_grad():
        outputs = model(patches)
        probs = torch.softmax(outputs, dim=1)
        ai_probs = probs[:, 1].cpu().numpy()

    confidence = np.mean(ai_probs)
    is_ai = confidence > 0.5

    return is_ai, confidence, img

# Streamlit UI
st.set_page_config(page_title="AI Image Detector", page_icon="üîç", layout="wide")

# Load model
model = load_model()
if model is None:
    st.stop()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Custom CSS
st.markdown("""
<style>
    .stProgress > div > div > div > div {
        background-color: #4CAF50;
    }
    .ai-progress {
        background-color: #f44336 !important;
    }
    .result-box {
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
        font-size: 16px;
    }
    .ai-result {
        background-color: #ffebee;
        border-left: 5px solid #f44336;
    }
    .human-result {
        background-color: #e8f5e9;
        border-left: 5px solid #4caf50;
    }
    .confidence-container {
        width: 100%;
        margin: 15px 0;
    }
    .confidence-labels {
        display: flex;
        justify-content: space-between;
        margin-bottom: 5px;
    }
    .confidence-meter {
        height: 25px;
        background: #e0e0e0;
        border-radius: 12px;
        overflow: hidden;
        position: relative;
    }
    .confidence-fill {
        height: 100%;
        position: absolute;
        transition: width 0.5s;
    }
    .human-fill {
        background: #4caf50;
        left: 0;
    }
    .ai-fill {
        background: #f44336;
        right: 0;
    }
</style>
""", unsafe_allow_html=True)

# Header
st.title("üîç AI Image Detector")
st.markdown("Upload any image to check if it was AI-generated or human-created")

# File upload
uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Uploaded Image")
        image_bytes = uploaded_file.read()
        image = Image.open(io.BytesIO(image_bytes))
        st.image(image, caption="Your uploaded image", use_column_width=True)

    with col2:
        st.subheader("Analysis Results")

        with st.spinner("Analyzing image..."):
            progress_bar = st.progress(0)
            for percent_complete in range(100):
                time.sleep(0.01)
                progress_bar.progress(percent_complete + 1)

            is_ai, confidence, processed_img = predict_image(image_bytes)

        if is_ai is not None:
            human_confidence = 1 - confidence

            # Display result
            if is_ai:
                st.markdown(f"""
                <div class="result-box ai-result">
                    üîç <b>Result:</b> This image appears to be <span style='color:#f44336'><b>AI-generated</b></span>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div class="result-box human-result">
                    üîç <b>Result:</b> This image appears to be <span style='color:#4caf50'><b>human-made</b></span>
                </div>
                """, unsafe_allow_html=True)

            # Confidence text
            st.markdown(f"""
            <b>Confidence:</b>
            <span style="color:#4caf50">{human_confidence*100:.1f}% human</span> /
            <span style="color:#f44336">{confidence*100:.1f}% AI</span>
            """, unsafe_allow_html=True)

            # Confidence meter
            st.markdown(f"""
            <div class="confidence-container">
                <div class="confidence-labels">
                    <span style="color:#4caf50">Human</span>
                    <span style="color:#f44336">AI</span>
                </div>
                <div class="confidence-meter">
                    <div class="confidence-fill human-fill" style="width:{human_confidence*100}%"></div>
                    <div class="confidence-fill ai-fill" style="width:{confidence*100}%"></div>
                </div>
            </div>
            """, unsafe_allow_html=True)

# Sidebar with info
st.sidebar.markdown("""
## About This Tool
This AI Image Detector analyzes visual patterns to determine whether an image was likely created by AI or human.

**How it works:**
1. Upload any JPG/PNG image
2. The model analyzes multiple patches of the image
3. Results show the probability of AI generation

**Tips for best results:**
- Use clear, high-quality images
- Works best on digital art and photorealistic images
- Results are estimates based on visual patterns
""")